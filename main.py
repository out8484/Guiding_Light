import os
from openai import OpenAI
import streamlit as st
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Initialize session state for API token
if 'github_token' not in st.session_state:
    st.session_state.github_token = None

# Streamlit app
st.title("ğŸ¤– GitHub Models AI ì±—ë´‡")

# Function to validate GitHub Token
def validate_token(token):
    import time
    max_retries = 3
    retry_delay = 1
    
    client = OpenAI(
        base_url="https://models.inference.ai.azure.com",
        api_key=token
    )
    
    for attempt in range(max_retries):
        try:
            # Make a minimal API call to check validity
            client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": "Test"}],
                max_tokens=1
            )
            return True, "ìœ íš¨í•œ í† í°ì…ë‹ˆë‹¤"
        except Exception as e:
            error_msg = str(e)
            # Check for connection errors
            if "connection" in error_msg.lower() or "timeout" in error_msg.lower():
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    retry_delay *= 2
                    continue
            return False, error_msg
            
    return False, "ì—°ê²° ì˜¤ë¥˜ê°€ ì§€ì†ë©ë‹ˆë‹¤. ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”."

# Function to get response from GitHub Models
def get_ai_response(question, github_token, language, age_group, gender):
    import time
    max_retries = 3
    retry_delay = 1
    
    # GitHub Models endpoint
    client = OpenAI(
        base_url="https://models.inference.ai.azure.com",
        api_key=github_token
    )
    
    system_prompt = f"""You are a helpful assistant. 
    Please answer in {language}. 
    The user is a {age_group} {gender}. 
    Tailor your answer to be appropriate for this demographic, using suitable tone, examples, and complexity."""

    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": question}
                ],
                temperature=1.0,
                max_tokens=4096,
                top_p=1.0
            )
            return {"success": True, "content": response.choices[0].message.content.strip()}
        except Exception as e:
            error_msg = str(e)
            # Check for connection errors
            if "connection" in error_msg.lower() or "timeout" in error_msg.lower():
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    retry_delay *= 2  # Exponential backoff
                    continue
            
            is_auth_error = "401" in error_msg or "unauthorized" in error_msg.lower() or "credentials" in error_msg.lower()
            return {"success": False, "content": error_msg, "is_auth_error": is_auth_error}
            
    return {"success": False, "content": "Failed to connect after multiple attempts. Please check your internet connection.", "is_auth_error": False}

# Function to generate image using Pollinations.ai (Free alternative)
def get_ai_image(prompt, github_token):
    try:
        # Pollinations.ai doesn't require an API key
        # We just need to construct the URL with the prompt
        import urllib.parse
        
        encoded_prompt = urllib.parse.quote(prompt)
        image_url = f"https://image.pollinations.ai/prompt/{encoded_prompt}"
        
        return {"success": True, "url": image_url}
    except Exception as e:
        return {"success": False, "error": str(e)}


# -----------------------------------------------------------------------------
# Educational Mode & Token Analysis Functions
# -----------------------------------------------------------------------------
import tiktoken

def count_tokens(text, model="gpt-4o"):
    """Returns the number of tokens in a text string."""
    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        encoding = tiktoken.get_encoding("cl100k_base")
    num_tokens = len(encoding.encode(text))
    return num_tokens

def get_token_ids(text, model="gpt-4o"):
    """Returns the list of token IDs."""
    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        encoding = tiktoken.get_encoding("cl100k_base")
    return encoding.encode(text)

def calculate_cost(input_tokens, output_tokens):
    """Calculates estimated cost for GPT-4o-mini (as of late 2024)."""
    # Pricing: Input $0.15 / 1M tokens, Output $0.60 / 1M tokens
    input_price = 0.15
    output_price = 0.60
    
    cost = (input_tokens / 1_000_000 * input_price) + (output_tokens / 1_000_000 * output_price)
    return cost

def visualize_tokens(text, model="gpt-4o"):
    """Visualizes tokens with alternating colors."""
    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        encoding = tiktoken.get_encoding("cl100k_base")
        
    tokens = encoding.encode(text)
    decoded_tokens = [encoding.decode([t]) for t in tokens]
    
    # Create HTML for visualization
    html_content = '<div style="line-height: 2.0; font-family: monospace;">'
    colors = ["#FFD700", "#ADD8E6", "#90EE90", "#FFB6C1", "#FFA500"] # Simple pastel colors
    
    for i, token_str in enumerate(decoded_tokens):
        # Escape HTML characters
        safe_token = token_str.replace("<", "&lt;").replace(">", "&gt;").replace("\n", "â†µ")
        color = colors[i % len(colors)]
        html_content += f'<span style="background-color: {color}; padding: 2px 4px; border-radius: 4px; margin-right: 2px; color: black;" title="Token ID: {tokens[i]}">{safe_token}</span>'
    html_content += '</div>'
    
    return html_content

# -----------------------------------------------------------------------------

# Sidebar for User Settings
with st.sidebar:
    st.header("ğŸ‘¤ ì‚¬ìš©ì ì„¤ì •")
    st.markdown("AIì˜ ì‘ë‹µ ë°©ì‹ì„ ì„¤ì •í•˜ì„¸ìš”")
    
    language = st.selectbox(
        "ì–¸ì–´ ì„ íƒ",
        options=["Korean (í•œêµ­ì–´)", "English (ì˜ì–´)"],
        index=0
    )
    
    age_group = st.selectbox(
        "ì—°ë ¹ëŒ€",
        options=["10ëŒ€", "20ëŒ€", "30ëŒ€", "40ëŒ€", "50ëŒ€", "60ëŒ€ ì´ìƒ"],
        index=1
    )
    
    gender = st.radio(
        "ì„±ë³„",
        options=["ë‚¨ì„±", "ì—¬ì„±", "ê¸°íƒ€"],
        index=0
    )
    
    st.divider()
    generate_image = st.checkbox("ğŸ–¼ï¸ ë‹µë³€ê³¼ í•¨ê»˜ ì´ë¯¸ì§€ ìƒì„±í•˜ê¸°", value=False, help="ì²´í¬í•˜ë©´ ë‹µë³€ ë‚´ìš©ì— ì–´ìš¸ë¦¬ëŠ” ì´ë¯¸ì§€ë¥¼ í•¨ê»˜ ìƒì„±í•©ë‹ˆë‹¤.")
    
    st.divider()
    # Educational Mode Toggle
    st.header("ğŸ“ AI í•™ìŠµ ëª¨ë“œ")
    educ_mode = st.toggle("AI ë™ì‘ ë¶„ì„ ëª¨ë“œ ì¼œê¸°", value=False, help="í† í° ì‚¬ìš©ëŸ‰, ë¹„ìš©, ì„ë² ë”©/ì¸ì½”ë”© ê³¼ì •ì„ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤.")
    
    st.divider()
    st.markdown("â„¹ï¸ **ì°¸ê³ :** ì„ íƒí•˜ì‹  ì„¤ì •ì— ë§ì¶° AIê°€ ë‹µë³€ì˜ í†¤ê³¼ ë‚´ìš©ì„ ì¡°ì ˆí•©ë‹ˆë‹¤.")

# Function to extract system prompt (refactored for reuse)
def get_system_prompt(language, age_group, gender):
    return f"""You are a helpful assistant. 
    Please answer in {language}. 
    The user is a {age_group} {gender}. 
    Tailor your answer to be appropriate for this demographic, using suitable tone, examples, and complexity."""

# Function to save token and date to .env file
def save_token_to_env(token):
    import datetime
    env_path = ".env"
    current_date = datetime.date.today().isoformat()
    
    # Read existing content
    if os.path.exists(env_path):
        with open(env_path, "r") as f:
            lines = f.readlines()
    else:
        lines = []
    
    # Update or add GITHUB_TOKEN and GITHUB_TOKEN_DATE
    token_line = f"GITHUB_TOKEN={token}\n"
    date_line = f"GITHUB_TOKEN_DATE={current_date}\n"
    
    new_lines = []
    token_updated = False
    date_updated = False
    
    for line in lines:
        if line.startswith("GITHUB_TOKEN="):
            new_lines.append(token_line)
            token_updated = True
        elif line.startswith("GITHUB_TOKEN_DATE="):
            new_lines.append(date_line)
            date_updated = True
        else:
            new_lines.append(line)
            
    if not token_updated:
        new_lines.append(token_line)
    if not date_updated:
        new_lines.append(date_line)
        
    # Write back to file
    with open(env_path, "w") as f:
        f.writelines(new_lines)

# Function to check token expiration
def check_token_expiration():
    import datetime
    token_date_str = os.getenv("GITHUB_TOKEN_DATE")
    
    if not token_date_str:
        return None, None
        
    try:
        token_date = datetime.date.fromisoformat(token_date_str)
        expiration_date = token_date + datetime.timedelta(days=30)
        today = datetime.date.today()
        
        days_remaining = (expiration_date - today).days
        
        return days_remaining, expiration_date
    except ValueError:
        return None, None

# Main Logic
token_from_env = os.getenv("GITHUB_TOKEN")

# Determine if we have a valid token in session
if 'github_token' not in st.session_state:
    st.session_state.github_token = None

    # Check if we just failed auth (Runtime auto-logout)
    if st.session_state.get('auth_failure_reset', False):
        st.warning("âš ï¸ ì¸ì¦ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ì¬ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        st.session_state.auth_failure_reset = False
    
    # Try to load from env ONLY if session is empty and we didn't just fail
    elif token_from_env and token_from_env != "your_github_token_here":
        # 1. First check if the token is expired by date
        days_left, exp_date = check_token_expiration()
        
        if days_left is not None and days_left <= 0:
            st.warning(f"âš ï¸ ì €ì¥ëœ í† í°ì˜ ìœ íš¨ê¸°ê°„ì´ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤ ({exp_date}). ë‹¨ê³„ë¥¼ ì§„í–‰í•˜ë ¤ë©´ ìƒˆë¡œìš´ í† í°ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # 2. If date is okay, validate with API
        else:
            with st.spinner("ğŸ”„ .env íŒŒì¼ì˜ í† í°ì„ í™•ì¸ ì¤‘ì…ë‹ˆë‹¤..."):
                clean_env_token = token_from_env.strip()
                is_valid, msg = validate_token(clean_env_token)
                if is_valid:
                    st.session_state.github_token = clean_env_token
                else:
                    st.warning(f"âš ï¸ ì €ì¥ëœ í† í°ì´ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”.\nì˜¤ë¥˜: {msg}")

# 2. If still no valid token, show input
if not st.session_state.github_token:
    st.warning("âš ï¸ ìœ íš¨í•œ GitHub Tokenì´ ì—†ìŠµë‹ˆë‹¤. ì•„ë˜ì— í† í°ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    with st.expander("â„¹ï¸ GitHub Token ë°œê¸‰ ë°©ë²• ë³´ê¸°"):
        st.markdown("""
        **GitHub Modelsìš© í† í° ë°œê¸‰ ë°©ë²•:**
        1. [GitHub Settings > Developer settings > Personal access tokens](https://github.com/settings/tokens)ë¡œ ì´ë™í•˜ì„¸ìš”.
        2. "Generate new token (classic)"ì„ í´ë¦­í•˜ì„¸ìš”.
        3. í† í° ì´ë¦„ì„ ì…ë ¥í•˜ê³  í•„ìš”í•œ ê¶Œí•œ(Scopes)ì„ ì„ íƒí•˜ì„¸ìš”.
        4. ìƒì„±ëœ í† í°ì„ ë³µì‚¬í•˜ì—¬ ì•„ë˜ì— ë¶™ì—¬ë„£ìœ¼ì„¸ìš”.
        """)
    
    with st.form(key="token_form"):
        token_input = st.text_input("GitHub Token ì…ë ¥:", type="password", placeholder="github_pat_...")
        col1, col2 = st.columns([1, 1])
        with col1:
            token_submit = st.form_submit_button("í™•ì¸ ë° ì €ì¥")
        with col2:
            reset_input = st.form_submit_button("ì…ë ¥ ì´ˆê¸°í™”", type="secondary")
            
    if reset_input:
        st.rerun()
        
    if token_submit and token_input:
        with st.spinner("ğŸ” í† í° í™•ì¸ ì¤‘..."):
            clean_token = token_input.strip()
            is_valid, msg = validate_token(clean_token)
            if is_valid:
                st.session_state.github_token = clean_token
                save_token_to_env(clean_token)
                st.success("âœ… í† í°ì´ í™•ì¸ë˜ê³  .env íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                st.rerun()
            else:
                st.error(f"âŒ ìœ íš¨í•˜ì§€ ì•Šì€ í† í°ì…ë‹ˆë‹¤: {msg}")

# 3. If we have a valid token, show the question form
if st.session_state.github_token:
    days_left, exp_date = check_token_expiration()
    if days_left is not None:
        if days_left <= 0:
            st.error(f"ğŸš¨ í† í° ìœ íš¨ê¸°ê°„(30ì¼)ì´ ì§€ë‚¬ìŠµë‹ˆë‹¤! ({exp_date} ë§Œë£Œ)")
            st.info("ìƒˆë¡œìš´ í† í°ì„ ë°œê¸‰ë°›ì•„ 'í† í° ì¬ì„¤ì •'ì„ í•´ì£¼ì„¸ìš”.")
        elif days_left <= 5:
            st.warning(f"âš ï¸ í† í° ë§Œë£Œê°€ {days_left}ì¼ ë‚¨ì•˜ìŠµë‹ˆë‹¤. ({exp_date} ë§Œë£Œ ì˜ˆì •)")
            
    col1, col2 = st.columns([3, 1])
    with col1:
        st.success("âœ… GitHub Tokenì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤")
    with col2:
        if st.button("ğŸ”„ í† í° ì¬ì„¤ì •"):
            st.session_state.github_token = None
            st.rerun()

    # Chat Interface
    with st.form(key="question_form", clear_on_submit=True):
        user_question = st.text_input("ğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", key="question_input", placeholder="ê¶ê¸ˆí•œ ë‚´ìš©ì„ ì…ë ¥í•˜ê³  Enterë¥¼ ëˆ„ë¥´ì„¸ìš”")
        submit_button = st.form_submit_button("ğŸš€ ì „ì†¡")

    if submit_button and user_question:
        # Create tabs for Normal View vs Educational View if enabled
        if educ_mode:
            tab1, tab2, tab3 = st.tabs(["ğŸ’¬ ì±„íŒ…", "ğŸ“Š í† í°/ë¹„ìš© ë¶„ì„", "ğŸ” ì¸ì½”ë”©/ì„ë² ë”© ì›ë¦¬"])
        else:
            tab1 = st.container()

        # 1. Chat Tab (Visualizing Inference Streaming)
        with tab1:
            st.write("### ğŸ“ ë‹µë³€:")
            message_placeholder = st.empty()
            full_response = ""
            
            # Streaming Logic
            client = OpenAI(
                base_url="https://models.inference.ai.azure.com",
                api_key=st.session_state.github_token
            )
            
            lang_code = language.split("(")[0].strip()
            system_prompt = get_system_prompt(lang_code, age_group, gender)
            
            try:
                # Streaming call
                stream = client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_question}
                    ],
                    temperature=1.0,
                    max_tokens=4096,
                    top_p=1.0,
                    stream=True  # ENABLE STREAMING
                )
                
                # Inference Simulation: Show tokens appearing one by one
                for chunk in stream:
                    if chunk.choices[0].delta.content is not None:
                        chunk_content = chunk.choices[0].delta.content
                        full_response += chunk_content
                        # Update UI with "cursor" effect to simulate typing
                        message_placeholder.markdown(full_response + "â–Œ")
                
                # Final clean output
                message_placeholder.markdown(full_response)
                
                # Generate Image if requested
                if generate_image:
                    with st.spinner("ğŸ¨ ì´ë¯¸ì§€ ìƒì„± ì¤‘..."):
                        image_prompt = f"Create an illustration representing this concept: {user_question}. Context: {full_response[:100]}"
                        image_result = get_ai_image(image_prompt, st.session_state.github_token)
                        if image_result["success"]:
                            st.image(image_result["url"], caption="AIê°€ ìƒì„±í•œ ì´ë¯¸ì§€")
                        else:
                            st.warning(f"âš ï¸ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {image_result['error']}")

            except Exception as e:
                st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                if "401" in str(e) or "unauthorized" in str(e).lower():
                    st.session_state.github_token = None
                    st.session_state.auth_failure_reset = True
                    st.rerun()

        # 2. Educational Tabs
        if educ_mode:
            # Prepare Data for Analysis
            input_text = system_prompt + "\n" + user_question
            input_tokens = count_tokens(input_text)
            output_tokens = count_tokens(full_response)
            total_tokens = input_tokens + output_tokens
            estimated_cost = calculate_cost(input_tokens, output_tokens)
            
            with tab2:
                st.header("ğŸ“Š í† í° ë° ë¹„ìš© ë¶„ì„")
                st.info("GitHub ModelsëŠ” í˜„ì¬ ë¬´ë£Œì§€ë§Œ, ì‹¤ì œ ì„œë¹„ìŠ¤(OpenAI API) ê¸°ì¤€ ì˜ˆìƒ ë¹„ìš©ì…ë‹ˆë‹¤.")
                
                col_a, col_b, col_c = st.columns(3)
                col_a.metric("ì…ë ¥ í† í°", f"{input_tokens}ê°œ", help="ì§ˆë¬¸ + ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í¬í•¨")
                col_b.metric("ì¶œë ¥ í† í°", f"{output_tokens}ê°œ", help="AIê°€ ìƒì„±í•œ ë‹µë³€")
                col_c.metric("ì´ í† í°", f"{total_tokens}ê°œ")
                
                st.metric("ğŸ’° ì˜ˆìƒ ë¹„ìš© (USD)", f"${estimated_cost:.6f}")
                
                st.subheader("ğŸ’¡ í† í°ì´ë€?")
                st.markdown("""
                - **í† í°(Token)**ì€ AIê°€ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê¸°ë³¸ ë‹¨ìœ„ì…ë‹ˆë‹¤.
                - ì˜ì–´ëŠ” ë³´í†µë‹¨ì–´ í•˜ë‚˜ê°€ 1í† í°, í•œêµ­ì–´ëŠ” ê¸€ì í•˜ë‚˜ê°€ 1~3í† í° ì •ë„ ë©ë‹ˆë‹¤.
                - ë¹„ìš©ì€ ì´ í† í° ìˆ˜ì— ë¹„ë¡€í•˜ì—¬ ì²­êµ¬ë©ë‹ˆë‹¤.
                """)

            with tab3:
                st.header("ğŸ” ì¸ì½”ë”© & ì„ë² ë”© ì‹œê°í™”")
                st.markdown("AIëŠ” ê¸€ìë¥¼ ì´í•´í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ëŒ€ì‹  **ìˆ«ì(Token ID)**ë¡œ ë³€í™˜í•˜ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
                
                st.subheader("1. ì…ë ¥ ì§ˆë¬¸ì˜ í† í°í™” (Tokenization)")
                st.markdown(visualize_tokens(user_question), unsafe_allow_html=True)
                
                st.subheader("2. ì •ìˆ˜ ì¸ì½”ë”© (Encoding)")
                st.markdown("ìœ„ì˜ ê° ì¡°ê°ë“¤ì€ ë‹¤ìŒê³¼ ê°™ì€ **ê³ ìœ í•œ ìˆ«ì ID**ë¡œ ì»´í“¨í„°ì— ì „ë‹¬ë©ë‹ˆë‹¤.")
                token_ids = get_token_ids(user_question)
                st.code(str(token_ids), language="json")
                
                st.subheader("3. ì„ë² ë”© (Embedding) ê°œë…")
                st.markdown("""
                > **ì„ë² ë”©ì´ë€?** 
                > ìœ„ì—ì„œ ë³¸ ìˆ«ì(Token ID) í•˜ë‚˜í•˜ë‚˜ë¥¼ **ìˆ˜ì²œ ê°œì˜ ì‹¤ìˆ˜(float)ë¡œ ì´ë£¨ì–´ì§„ ë²¡í„°** ì¢Œí‘œë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. 
                > ì´ë¥¼ í†µí•´ AIëŠ” ë‹¨ì–´ì˜ **'ì˜ë¯¸'**ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ì´í•´í•˜ê²Œ ë©ë‹ˆë‹¤.
                """)
                st.image("https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.png", caption="ì„ë² ë”© ë²¡í„° ì‹œê°í™” ì˜ˆì‹œ (ì¶œì²˜: OpenAI)", width=600)

    elif submit_button:
        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

